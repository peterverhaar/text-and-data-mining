{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Many of the functionalities that can be offered by text analysis tools are based on counts of the smaller linguistic units that occur within texts, such as their words or their sentences. \n",
    "This preparatory process, in which the program divides the linear texts into discrete units, is generally referred to as “segmentation” or “tokenisation”. Segmentation generally takes place on the basis of the spaces, the punctuation marks and the line breaks that can be found within texts. Utilising such notational conventions, which can be referred to as ‘soft markup’, text mining applications can be developed for the recognition of units such as words, sentences or paragraphs. \n",
    "\n",
    "The individual words that are found are commomly referred to as “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. Frequency lists, counting occurrences of types, often form the basis for further statistical analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a function named `tokenise()`. It demand a certain string as input. This may be a sentences, or a whole paragraph. The function returns a list containing all the individual words found in this string.  \n",
    "\n",
    "The first line of the function creates an empty list which will eventually receive all the tokens in this list. In the first few lines, the sting that is supplied is converted into lower case. All  occurrences of the em-dash (i.e. two consecutive hyphenss) are also surrounded by spaces. In some texts, such em-dashed are used to separate words. \n",
    "\n",
    "Next, the string is divided into its separate words using the `split()` method from the `re` module. All leading and trailing punctuation is removed from the words that are found in this way, using the standard `strip` method, which is available for all strings. The `strip()` method in the code below removes all the symbols that are predefined in the Python language as `string.punctuation`. \n",
    "\n",
    "It was decided, finally, that words should minimally contain one alphabetic character. The condition given after the `if` keyword stipulates that substrings can only be appended to the `tokens` list when they contain at least one character from our alphabet, either in upper or lower case. Strings which only consist of numbers are disregarded in this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# function to tokenise a string into words\n",
    "def word_tokenise( text ):\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub( '--' , ' -- ' , text)\n",
    "    words = re.split( r'\\s+' , text )\n",
    "    for w in words:\n",
    "        w = w.strip( string.punctuation )\n",
    "        if re.search( r\"[a-zA-Z]\" , w ):\n",
    "            tokens.append(w)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'How many words are there in this sentence?'\n",
    "\n",
    "#t = text('Corpus/ARoomWithaView.txt')\n",
    "\n",
    "words = word_tokenise(sentence)\n",
    "\n",
    "    \n",
    "print( f'The sentence contains {len(words)} tokens:' )\n",
    "\n",
    "for w in words:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenise()` function can subsequently be used to calculate word frequencies. \n",
    "\n",
    "To keep track of the word counts, it can be very useful to work with a Python dictionary. As explained in the tutorial, a dictionary is variable which can hold value pairs. In this situation, we also want to store pairs of values: \n",
    "\n",
    "* As keys: the types found in the text and \n",
    "* As values: the number of times thes types occur. \n",
    "\n",
    "The code below defines the words (i.e. the types) as the indices of the dictionary named `freq`, and the values that these indices are associated with represent the number of times these words are found in the text. \n",
    "\n",
    "The following code reads in a specific text line by line. It firstly tokenises each line, using the `tokenise()` method. Each word that is found in this way will become an index in the `freq` dictionary. At the first occurrence of this word, the word will be assigned the value 1. The value will be updated, however, with every new occurrence of the word in the text. The program does this for each individual word. Once Python has read the full text, it will have data about the occurrences of all the words in this text. \n",
    "\n",
    "The final few lines print the 30 most frequent words. The number of words to be shown can be manipulated by changing the value of the variable named `max`.  \n",
    "\n",
    "To calculate a frequency list for one of the texts in your own corpus, you obviously need to change the value of the variable named `file`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "\n",
    "# create variable for the name of the folder containing your texts\n",
    "dir = 'Corpus'\n",
    "\n",
    "# create variable for the name of file you want to analyse\n",
    "file = 'ARoomWithAView.txt'\n",
    "# create a dictionary which will count the tokens\n",
    "# word as index, and frequency as value\n",
    "freq = dict()\n",
    "\n",
    "# read the text\n",
    "text = open( join( dir, file ) )\n",
    "\n",
    "\n",
    "# Read the text, and iterate through the text line by line. \n",
    "for line in text:\n",
    "    # Tokenise each line, and update the dictionary as you go along\n",
    "    words = word_tokenise( line )  \n",
    "    for w in words:\n",
    "        freq[w] = freq.get( w, 0 ) + 1\n",
    "         \n",
    "\n",
    "            \n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "            \n",
    "count = 0 \n",
    "max = 30 \n",
    "\n",
    "for word in reversed( sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have managed to run the code above, you shall probably see that the list that is created is headed by so-called function words. These are words such as articles, prepositions and pronouns. These words are important for producting grammatically correct sentences, but they mostly have little expessive value by themselves. If you are interested in studying the contents of a text, it can be useful to remove such frequently used function words. The `nltk` library can help you with this. It contain a module named `stopwords`, which includes a predefined list of frequently used function words. \n",
    "\n",
    "The code below gives a demonstration of this. It firstly creates a list named `stopwords`, on the basis of the predefined list from the `nltk` library. Next, it created a copy of the `freq` dictionary that was created earlier, and it saves the copy under the name `freq_copy`. Next, while iterating through the `freq_copy` dictionary, all the words that are on the list of stopwords are removed from the original `freq` dictionary. \n",
    "\n",
    "When you run the code below, you should see an updated list, without the frequently used function words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from os.path import join\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "text = open( join( dir, file ) )\n",
    "\n",
    "for line in text:\n",
    "    words = word_tokenise( line )  \n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            freq[w] = freq.get( w, 0 ) + 1\n",
    "            \n",
    "max = 30 \n",
    "count = 1\n",
    "\n",
    "for word in reversed( sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also make use of lists of stopwords that have been defined by other teams of researchers. The *Information Retrieval Group* at the University of Glasgow has compiled [a useful list of stop words](http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words) as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with word frequencies into a word cloud. \n",
    "\n",
    "To make sure that the word cloud works with the correct dictionary, firstly run the code in one of the cells above, and run the code for making the word cloud immediately after this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above produced an error message, this may be caused by the fact that wordcloud module has not been inestalled yet on your computer. To get the code above to work, you may need to install the wordcloud module using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion graphs\n",
    "\n",
    "Frequencies can also be clarified in dispersion graphs. Instead of giving information about the total number of occurrences, dispersion graphs (or distribution graphs) indicate the number of occurrences in separate sections of the texts. While it is possible to calculate frequencies within, for example, the separate chapters of a novel, such dispersion graphs often work with random segments, simply created by chopping up the text into chunks of a given number of words. \n",
    "\n",
    "To produce such graphs, a full text firstly needs to be divided into segments. The graph that is generated provides information on the frequency of a particular word within each of these segments. The graph can be used to locate those sections in which a given term is used most frequently.\n",
    "\n",
    "In the code below, the method `divideIntoSegments()` demands two parameters. The first of these is the text to be analysed. Via the second parameter, you can specify the number of segments that need to be created. The number of segments als determines the size of the segments. The number of these segments are calculated by dividing the total number of tokens by the number of segments. \n",
    "\n",
    "Note this code also makes use of the `word_tokenise()` function that was discussed above. Make sure that the cell containing this function has been executed before running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def divideIntoSegments( full_text , nr_segments ):\n",
    "\n",
    "    segments = []\n",
    "\n",
    "\n",
    "    all_words = word_tokenise( full_text )\n",
    "\n",
    "    segmentSize = int( len(all_words) / nr_segments )\n",
    "\n",
    "    count_words = 0\n",
    "    text = ''\n",
    "\n",
    "    for word in all_words:\n",
    "        count_words += 1\n",
    "        text += word + ' '\n",
    "\n",
    "        ## This line below used the modulo operator:\n",
    "        ## We can use it to test if the first number is\n",
    "        ## divisible by the second number\n",
    "        if count_words % segmentSize == 0:\n",
    "            segments.append(text.strip())\n",
    "            text = ''\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'MobyDick.txt'\n",
    "\n",
    "text_file = open( join( dir, file ) )\n",
    "full_text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "segments = divideIntoSegments( full_text , 30 )\n",
    "\n",
    "data = dict()\n",
    "\n",
    "count = 0\n",
    "for s in segments:\n",
    "    count += 1\n",
    "    hits = 0 \n",
    "    for w in word_tokenise(s):\n",
    "        if re.search( r'\\bwhale*\\b' , w , re.IGNORECASE ):\n",
    "            hits += 1\n",
    "    data[ count ] = hits\n",
    "    \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( list(data.keys() ) , list( data.values() ) , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Section')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax.set_title( 'Dispersion graph for \"Moby Dick\" ' , fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions that have been discussed in this notebook (`word_tokenise()` and `divideIntoSegments()`) can also be found in the module named `tdm`, so that they can easily be reused in other programs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is important to emphasise that there are many other libraries and modules that enable you to tokenise a texts. The `nltk` library, for instance, includes a method named `word_tokenize()`, which can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ARoomWithAView.txt'\n",
    "path = join( dir, text )\n",
    "\n",
    "with open( path , encoding = 'utf-8' ) as file:\n",
    "    full_text = file.read()\n",
    "    full_text = full_text.lower()\n",
    "\n",
    "words = word_tokenize(full_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
