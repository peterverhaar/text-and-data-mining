{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Many of the functionalities that can be offered by text analysis tools are based on counts of the smaller linguistic units that occur within texts, such as their words or their sentences. \n",
    "This preparatory process, in which the program divides the linear texts into discrete units, is generally referred to as “segmentation” or “tokenisation”. Segmentation generally takes place on the basis of the spaces, the punctuation marks and the line breaks that can be found within texts. Utilising such notational conventions, which can be referred to as ‘soft markup’, text mining applications can be developed for the recognition of units such as words, sentences or paragraphs. \n",
    "\n",
    "The individual words that are found are commomly referred to as “**tokens**\". When this full list is deduplicated, leaving only the unique words, the items in such lists are called “**types**”. Frequency lists, counting occurrences of types, often form the basis for further statistical analyses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a function named `tokenise()`. It demand a certain string as input. This may be a sentences, or a whole paragraph. The function returns a list containing all the individual words found in this string.  \n",
    "\n",
    "The first line of the function creates an empty list which will eventually receive all the tokens in this list. In the first few lines, the sting that is supplied is converted into lower case. All  occurrences of the em-dash (i.e. two consecutive hyphenss) are also surrounded by spaces. In some texts, such em-dashed are used to separate words. \n",
    "\n",
    "Next, the string is divided into its separate words using the `split()` method from the `re` module. All leading and trailing punctuation is removed from the words that are found in this way, using the standard `strip` method, which is available for all strings. The `strip()` method in the code below removes all the symbols that are predefined in the Python language as `string.punctuation`. \n",
    "\n",
    "It was decided, finally, that words should minimally contain one alphabetic character. The condition given after the `if` keyword stipulates that substrings can only be appended to the `tokens` list when they contain at least one character from our alphabet, either in upper or lower case. Strings which only consist of numbers are disregarded in this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# function to tokenise a string into words\n",
    "def word_tokenise( text ):\n",
    "    tokens = []\n",
    "    text = text.lower()\n",
    "    text = re.sub( '--' , ' -- ' , text)\n",
    "    words = re.split( r'\\s+' , text )\n",
    "    for w in words:\n",
    "        w = w.strip( string.punctuation )\n",
    "        if re.search( r\"[a-zA-Z]\" , w ):\n",
    "            tokens.append(w)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'How many words are there in this sentence?'\n",
    "\n",
    "#t = text('Corpus/ARoomWithaView.txt')\n",
    "\n",
    "words = word_tokenise(sentence)\n",
    "\n",
    "    \n",
    "print( f'The sentence contains {len(words)} tokens:' )\n",
    "\n",
    "for w in words:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tokenise()` function can subsequently be used to calculate word frequencies. \n",
    "\n",
    "To keep track of the word counts, it can be very useful to work with a Python dictionary. As explained in the tutorial, a dictionary is variable which can hold value pairs. In this situation, we also want to store pairs of values: \n",
    "\n",
    "* As keys: the types found in the text and \n",
    "* As values: the number of times thes types occur. \n",
    "\n",
    "The code below defines the words (i.e. the types) as the indices of the dictionary named `freq`, and the values that these indices are associated with represent the number of times these words are found in the text. \n",
    "\n",
    "The following code reads in a specific text line by line. It firstly tokenises each line, using the `tokenise()` method. Each word that is found in this way will become an index in the `freq` dictionary. At the first occurrence of this word, the word will be assigned the value 1. The value will be updated, however, with every new occurrence of the word in the text. The program does this for each individual word. Once Python has read the full text, it will have data about the occurrences of all the words in this text. \n",
    "\n",
    "The final few lines print the 30 most frequent words. The number of words to be shown can be manipulated by changing the value of the variable named `max`.  \n",
    "\n",
    "To calculate a frequency list for one of the texts in your own corpus, you obviously need to change the value of the variable named `file`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import tdm\n",
    "\n",
    "# create variable for the name of the folder containing your texts\n",
    "dir = 'Corpus'\n",
    "\n",
    "# create variable for the name of file you want to analyse\n",
    "file = 'ARoomWithAView.txt'\n",
    "# create a dictionary which will count the tokens\n",
    "# word as index, and frequency as value\n",
    "freq = dict()\n",
    "\n",
    "# read the text\n",
    "text = open( join( dir, file ) )\n",
    "\n",
    "\n",
    "# Read the text, and iterate through the text line by line. \n",
    "for line in text:\n",
    "    # Tokenise each line, and update the dictionary as you go along\n",
    "    words = word_tokenise( line )  \n",
    "    for w in words:\n",
    "        freq[w] = freq.get( w, 0 ) + 1\n",
    "         \n",
    "\n",
    "\n",
    "            \n",
    "count = 0 \n",
    "max = 30 \n",
    "\n",
    "for word in reversed( tdm.sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have managed to run the code above, you shall probably see that the list that is created is headed by so-called function words. These are words such as articles, prepositions and pronouns. These words are important for producting grammatically correct sentences, but they mostly have little expessive value independently. \n",
    "\n",
    "If you are interested in studying the contents of a text, it can be useful to remove such frequently used function words. One option is to download a text file containing stopwords from the web and to copy all of these words into a list. The code below copies stopwords from a file created by the [*Information Retrieval Group*](http://ir.dcs.gla.ac.uk/) at the University of Glasgow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "response = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words')\n",
    "\n",
    "stopwords = []\n",
    "if response.status_code == 200:\n",
    "    response.encoding = 'utf-8'\n",
    "    contents = response.text\n",
    "    lines = re.split(r'\\n' , contents)\n",
    "    for word in lines:\n",
    "        if re.search( r'\\w' , word ):\n",
    "            stopwords.append(word.strip())\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can make use of the module named `stopwords` from the `nltk` library. The code that follows creates a list named `stopwords`, on the basis of the predefined list from the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is is a revision of the code that was given earlier for calculating word frequencies. The only difference is that there is a condition in the second for-loop. Words will be added to the dictionary named `freq` only if it is *not* in the list of stopwords   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import *\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "text = open( join( dir, file ) )\n",
    "\n",
    "for line in text:\n",
    "    words = word_tokenise( line )  \n",
    "    for w in words:\n",
    "        if w not in stopwords:\n",
    "            freq[w] = freq.get( w, 0 ) + 1\n",
    "            \n",
    "max = 30 \n",
    "count = 1\n",
    "\n",
    "for word in reversed( sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( '{}. {} => {}'.format( count , word , freq[word] ) )\n",
    "    if count == max:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with word frequencies into a word cloud. \n",
    "\n",
    "To make sure that the word cloud works with the correct dictionary, firstly run the code in one of the cells above to ensure that the dictionaru naed `freq` actually exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above produced an error message, this may be caused by the fact that wordcloud module has not been inestalled yet on your computer. To get the code above to work, you may need to install the wordcloud module using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion graphs\n",
    "\n",
    "Frequencies can also be clarified in dispersion graphs. Instead of giving information about the total number of occurrences, dispersion graphs (or distribution graphs) indicate the number of occurrences in separate sections of the texts. While it is possible to calculate frequencies within, for example, the separate chapters of a novel, such dispersion graphs often work with random segments, simply created by chopping up the text into chunks of a given number of words. \n",
    "\n",
    "To produce such graphs, a full text firstly needs to be divided into segments. The graph that is generated provides information on the frequency of a particular word within each of these segments. The graph can be used to locate those sections in which a given term is used most frequently.\n",
    "\n",
    "In the code below, the method `divideIntoSegments()` demands two parameters. The first of these is the text to be analysed. Via the second parameter, you can specify the number of segments that need to be created. The number of segments als determines the size of the segments. The number of these segments are calculated by dividing the total number of tokens by the number of segments. \n",
    "\n",
    "Note this code also makes use of the `word_tokenise()` function that was discussed above. Make sure that the cell containing this function has been executed before running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def divideIntoSegments( full_text , nr_segments ):\n",
    "\n",
    "    segments = []\n",
    "\n",
    "\n",
    "    all_words = word_tokenise( full_text )\n",
    "\n",
    "    segmentSize = int( len(all_words) / nr_segments )\n",
    "\n",
    "    count_words = 0\n",
    "    text = ''\n",
    "\n",
    "    for word in all_words:\n",
    "        count_words += 1\n",
    "        text += word + ' '\n",
    "\n",
    "        ## This line below used the modulo operator:\n",
    "        ## We can use it to test if the first number is\n",
    "        ## divisible by the second number\n",
    "        if count_words % segmentSize == 0:\n",
    "            segments.append(text.strip())\n",
    "            text = ''\n",
    "    return segments\n",
    "\n",
    "\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'MobyDick.txt'\n",
    "\n",
    "text_file = open( join( dir, file ) )\n",
    "full_text = text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "segments = divideIntoSegments( full_text , 30 )\n",
    "\n",
    "data = dict()\n",
    "\n",
    "count = 0\n",
    "for s in segments:\n",
    "    count += 1\n",
    "    hits = 0 \n",
    "    for w in word_tokenise(s):\n",
    "        if re.search( r'\\bwhale*\\b' , w , re.IGNORECASE ):\n",
    "            hits += 1\n",
    "    data[ count ] = hits\n",
    "    \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig = plt.figure( figsize=( 12 , 6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot( list(data.keys() ) , list( data.values() ) , color = '#930d08' , linestyle = 'solid')\n",
    "\n",
    "ax.set_xlabel('Section')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "ax.set_title( 'Dispersion graph for \"Moby Dick\" ' , fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions that have been discussed in this notebook (`word_tokenise()` and `divideIntoSegments()`) can also be found in the module named `tdm`, so that they can easily be reused in other programs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tokenisers\n",
    "\n",
    "Finally, it is important to emphasise that there are many other libraries and modules that enable you to tokenise a texts. The `nltk` library, for instance, includes a method named `word_tokenize()`, which can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ARoomWithAView.txt'\n",
    "path = join( dir, text )\n",
    "\n",
    "with open( path , encoding = 'utf-8' ) as file:\n",
    "    full_text = file.read()\n",
    "    full_text = full_text.lower()\n",
    "\n",
    "words_nltk = word_tokenize(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `word_tokenize()` method leaves the case at it is in the original text. If you want to be able to calculate frequencies in a case-insensitive manner, you need to convert every word either to lower case (using the `lower()` method) or to upper case (using `upper()`). \n",
    "\n",
    "There are a number of differences, however, in the word tokenisation process. We can compare the two approaches by comparing the word lists that are created using `nltk` to the words that are recognised by the `word_tokenise()` function that was discussed at the beginning of this notebook. The code of this function has been copied and pasted into the `tdm` module, whichis imported in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdm\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ARoomWithAView.txt'\n",
    "path = join( dir, text )\n",
    "\n",
    "with open( path , encoding = 'utf-8' ) as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = tdm.word_tokenise(full_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important different between the two approaches is that `nltk` treats all the punction marks as separate tokens. When you run the code below, you should see many occurrences of semi-colons, dots, commans and quotes. Arguably, such punctuation marks should not be in a frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_nltk:\n",
    "    if word not in words:\n",
    "        print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another difference is that `nltk` aims to separate the genitival 's' from the tokens such as the following:\n",
    "\n",
    "* people's \n",
    "* father's\n",
    "* child's\n",
    "\n",
    "`nltk` likewise aims to separate the various parts in contracted verb forms. The `word_tokenise()` function retained character sequences such as `don't`, `i'm` and `weren't` as tokens in their own right. `nltk` aims to separate the stem form the rest of such tokens, but this also results in tokens such as \n",
    "\n",
    "* 'm\n",
    "* 'nt\n",
    "* 's\n",
    "* 't\n",
    "\n",
    "The question whether words in the genitival forms and contracted verb forms must be counted separately is of course open to debate. It is useful to be aware of the different approaches that may be followed while tokenising words, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word not in words_nltk:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
