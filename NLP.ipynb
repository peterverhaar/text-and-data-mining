{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Natural Language Processing* is an interdisciplinary area of research bringning together insights from fields such as artifical intelligence, computational linguistics, statistics and computer science. The ultimate aim of NLP is to enable computers to understand and to process the natural langauges that are spoken and written by human beings. Researchers in the field of NLP have developd sophisticated tools and algorithms for the recognition of grammatical and syntactic categories, or for the conversion of inflected word forms into their dictionary forms, among many other purposes. Current more advanced research focuses the development of software for tasks such as named entity recognition, machine translation, or unsupervised summarisation. Tasks such as these all demand an understanding not only of the grammar and the syntax, but also of the logical structure and the semantic contents of the text.\n",
    "\n",
    "\n",
    "## Nltk\n",
    "\n",
    "One of the ways in which you can analyse natural languages in Python is by making use of nltk, the *Natural Language Toolkit* (nltk). This library can be imported as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having imported nltk, you can also import specific methods from this library. \n",
    "\n",
    "The methods thar are discussed in this tutorial make use of a number of additional resources which are not installed by default. If you have never used `nltk` before, you need to run the code below to install the relevant components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "\n",
    "As was explained earlier, tokenisation is a process in which a full linear text is broken down into smaller linguistic units, such as words, sentences of paragraphs. In `nltk`, you can use the method `word_tokenize()` to tokenise a text into words. The method `sent_tokenize()` can divide a text into sentences. The cell below contains an illustration of how these methods can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "quote = '''\n",
    "In the late summer of that year we lived in a house in a village that looked across the river and the plain to the mountains. In the bed of the river there were pebbles and boulders, dry and white in the sun, and the water was clear and swiftly moving and blue in the channels. Troops went by the house and down the road and the dust they raised powdered the leaves of the trees. The trunks of the trees too were dusty and the leaves fell early that year and we saw the troops marching along the road and the dust rising and leaves, stirred by the breeze, falling and the soldiers marching and afterward the road bare and white except for the leaves.\n",
    "'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "sentences = sent_tokenize(quote)\n",
    "\n",
    "print( f'The fragmenst contains { len(words) } words, and { len(sentences) } sentences.\\n' )\n",
    "\n",
    "\n",
    "for w in words:\n",
    "    print(w)\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The method `word_tokenise()` divides the string into words on the basis of spaces. When you simply count the number of tokens found using this method, the results may sometimes be misleading. Interpunction marks are viewed as separate tokens as well, and, if you don't take any measure to exlude these punctuation marks, these are included in the counts as well. In most of the following examples will actually use the method `word_tokenise()` from the `tdm` module instead. \n",
    "\n",
    "\n",
    "## Part of speech tagging\n",
    "\n",
    "Part of speech (POS) taggers are applications which can produce data about the syntactic categories of words. Once you have imported the nltk library, you can generate such POS tags by making use of the `pos_tag()` method. This method demands a list of words as a parameter. \n",
    "\n",
    "`pos_tag()` is typically used in combination with a word tokenisation method. Teh output of this latter function can then be used as input to the `pos_tag()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tdm\n",
    "\n",
    "quote = '''\n",
    "The studio was filled with the rich odour of roses, and when the light summer wind stirred amidst the trees of the garden, there came through the open door the heavy scent of the lilac, or the more delicate perfume of the pink-flowering thorn.\n",
    "'''\n",
    "\n",
    "words = tdm.word_tokenise(quote)\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for p in pos:\n",
    "    print(p[0] + ' => ' + p[1] )\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pos_tag()` methods returns a composite variable with two values. More specifically, it is a data structure that is called a *tuple*. The first value is the word that was tagged and the second value is the POS tag that was assigned to this word. You can access these values individually using square brackets. \n",
    "\n",
    "The meaning of all of the POS tags can be displayed by printing the output of the `nltk.help.upenn_tagset()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( nltk.help.upenn_tagset() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of these POS codes can also be [found online](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
    "\n",
    "## Lemmatisation \n",
    "\n",
    "English verbs can be used, in the past tense, in the present tense, in the continuous form or in the perfect form, and these different forms can evidently make it more difficult to search systematically for occurrences of a specific verb. In this context, lemmatisation can offer a solution.\n",
    "\n",
    "Lemmatisation is a process in which the conjugated forms of the words that are found in a text are converted into their base dictionary form. This base form is referred to as the lemma. \n",
    "In many cases, the manner in which words are to be lemmatised depends on their contexts. Certain homonyms may either be verbs or nouns, for instance, and, depending on their usage, they should be lemmatised to different forms. \n",
    "\n",
    "The `lemmatize()` method, inside the `WordNetLemmatizer` module of the `nltk` library, can be used in combination with POS tags. The first parameter of this needs to be the word to be lemmatised. To improve the results of this method, you can optionally supply a the POS tag as a second parameter. Unlike `pos_tag()`, however, the `lemmatize()` method does not make use of the Penn Treebank codes but of the POS codes that have been defined for `wordnet`. \n",
    "\n",
    "The code below firstly tokenises the words in a given sentence using `word_tokenise`. Next, the code generates the POS codes using `pos_tag`. The Penn Treebank codes produced by this method are converted to `wordnet` codes used a new function named `ptb_to_wordnet()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tdm import word_tokenise\n",
    "import re\n",
    "\n",
    "\n",
    "def ptb_to_wordnet(PTT):\n",
    "\n",
    "    if PTT.startswith('J'):\n",
    "        ## Adjective\n",
    "        return 'a'\n",
    "    elif PTT.startswith('V'):\n",
    "        ## Verb\n",
    "        return 'v'\n",
    "    elif PTT.startswith('N'):\n",
    "        ## Noune\n",
    "        return 'n'\n",
    "    elif PTT.startswith('R'):\n",
    "        ## Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "quote = \"It was the best of times, it was the worst of times\"\n",
    "words = word_tokenise(quote)\n",
    "pos = nltk.pos_tag(words)\n",
    "\n",
    "for i in range( 0 , len(words) ):\n",
    "    word_found = words[i]\n",
    "    posTag = ptb_to_wordnet( pos[i][1] )\n",
    "    \n",
    "    if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "        lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "        print( f'{word_found} => {lemma}' )\n",
    "    else:\n",
    "        lemma = lemmatiser.lemmatize( words[i] )\n",
    "        print( f'{word_found} => {lemma}' )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual analysis using nltk\n",
    "\n",
    "As explained, the `sent_tokenize()` method from the `nltk` library can be used to divide a text into its separate sentences. If we divide the total number of words by the total number of sentences, we get a number which indicates the average length of the sentences, or the average number of words per sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from tdm import word_tokenise\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'HeartofDarkness.txt'\n",
    "path = join( dir , file )\n",
    "\n",
    "\n",
    "file_handler = open( path , encoding = 'utf-8' )\n",
    "full_text = file_handler.read()\n",
    "s = sent_tokenize(full_text)\n",
    "w = word_tokenise(full_text)\n",
    "words_per_sentence = len(w) / len(s)\n",
    "\n",
    "print( f'{file} contain { len(s) } sentences and { len(w) } words.' )\n",
    "print( f'The sentences contain {words_per_sentence} words on average.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the POS tagger in `nltk`, you can also develop a specific type of frequency list: one that only includes the words in a specific grammatical category. \n",
    "\n",
    "The code below tries to find all the adjectives that are used in given a text. In the Penn Treebank tag set, adjectives be tagged as 'JJ' (regular adjecives), 'JJR' (adjectives in the comparative form) or 'JJS' (superatives). \n",
    "\n",
    "The code creates a list named `tags` which specifies the categories of words that need to be counted. In the lines that follow, the text is processed sentence by sentence, using the `sent_tokenize()` method. All the words which, according to the POS tagger, belong to one of the categories that have been listed in `tags`, are appended to the `tokens` list. By the end of this code, the list `tokens` contains all the adjectives that were found in this way. \n",
    "\n",
    "Using a similar approach, you could make a list of all the adverbs (labelled 'RB', 'RBR' or 'RBS') in the text, or all the nouns in the text (labelled 'NN', 'NNS', 'NNP' or 'NNPS'). To do this, you obviously need to change the tags listed in the list `tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tdm import word_tokenise\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'HeartofDarkness.txt'\n",
    "\n",
    "## tags used for adjectives\n",
    "tags = [ 'JJS' ]\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'HeartofDarkness.txt'\n",
    "path = join( dir , file )\n",
    "\n",
    "\n",
    "file_handler = open( path , encoding = 'utf-8' )\n",
    "full_text = file_handler.read()\n",
    "\n",
    "print('Analysing POS tags ... \\n')\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "for s in sentences:\n",
    "    words = word_tokenise(s.lower() )\n",
    "    pos = nltk.pos_tag(words)\n",
    "\n",
    "    for w in pos:\n",
    "        token = w[0]\n",
    "        tag = w[1]\n",
    "        if tag in tags:\n",
    "            if len(token) > 1:\n",
    "                freq[token] = freq.get( token , 0 ) + 1 \n",
    "            \n",
    "def sortedByValue( dict ):      \n",
    "    return sorted( dict , key=lambda x: dict[x])             \n",
    "            \n",
    "\n",
    "for word in reversed( sortedByValue( freq ) ):\n",
    "    print( f'{word} => {freq[word]} ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability \n",
    "\n",
    "Pythons's `nltk` library does not include a method that can count the number of syllables in a word, surprisingly. To address this lacuna, you can make use of the method `countSyllables()` that is given below. As its own only parameter, the function demands a single English word. The code aims to count the number of syllables in a given word, on the basis of a regular expression.\n",
    "\n",
    "A demonstration is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countSyllables( word ):\n",
    "    pattern = \"e?[aiou]+e*|e(?!d$|ly).|[td]ed|le$|ble$|a$|y$\"\n",
    "    syllables = re.findall( pattern , word )\n",
    "    return len(syllables)\n",
    "\n",
    "\n",
    "print( countSyllables(\"beauty\") )\n",
    "print( countSyllables(\"believe\") )\n",
    "print( countSyllables(\"university\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we divide the total number of syllables in a text by the total number of tokens, the number that results gives an indication of the average length of the words (if measured in the number of syllables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "import nltk\n",
    "from tdm import word_tokenise , countSyllables\n",
    "\n",
    "nr_tokens = 0 \n",
    "nr_syllables = 0 \n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'HeartOfDarkness.txt'\n",
    "path = join( dir , file )\n",
    "\n",
    "with open(path , encoding = 'utf-8') as file_handler:\n",
    "    full_text = file_handler.read()\n",
    "\n",
    "word = word_tokenise(full_text)\n",
    "for w in word:\n",
    "    if re.search( r'\\w' , w):\n",
    "        nr_tokens += 1\n",
    "        nr_syllables += countSyllables( w )\n",
    "    \n",
    "print( f'{file}:\\n\\nTotal number of words: {nr_tokens}' )\n",
    "print( f'Total number of syllables: {nr_syllables}' )\n",
    "print( f'Average number of syllables: {nr_syllables / nr_tokens}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data on the average sentence lengths and on the average word lengths are commonly used in formulae developed to assess the overall complexity of a text. The Flesch–Kincaid Grade Level Formula, for instance, uses these two numbers in a formula which can ultimately be used to assess the number of years of education that is required to comprehend the text. \n",
    "\n",
    "The Flesch-Kincaid equation is as follows:\n",
    "\n",
    "fk = (0.39 * asl) + (11.8 x asw) - 15.59\n",
    "\n",
    "In this equation, 'asl' stands for the average senetnce length, which can in turn be calculated by dividing the total number of words by the number of sentences. \n",
    "\n",
    "'asw' stands for the average number of syllables per word. We can calculate this number by dividing the total number of syllables by the total number of words.  \n",
    "\n",
    "For more information, see, for instance: https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from tdm import word_tokenise , countSyllables\n",
    "from nltk import sent_tokenize\n",
    "import re\n",
    "\n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "path = join( dir , file )\n",
    "\n",
    "file_handler = open( path , encoding = 'utf-8' )\n",
    "full_text = file_handler.read()\n",
    "\n",
    "s = sent_tokenize(full_text)\n",
    "w = word_tokenise(full_text)\n",
    "\n",
    "asl = len(w) / len(s)\n",
    "\n",
    "nr_syllables = 0\n",
    "for word in w:\n",
    "    if re.search( r'\\w' , word):\n",
    "        nr_syllables += countSyllables( word )\n",
    "\n",
    "asw = nr_syllables / len(w)\n",
    "\n",
    "print( f'Average number of words per sentence: {asl}')\n",
    "print( f'Average number of syllables per words: {asw}')\n",
    "\n",
    "\n",
    "fk = 0.39 * ( asl )\n",
    "fk = fk + 11.8 * ( asw )\n",
    "fk = fk - 15.59\n",
    "fk = round(fk,1)\n",
    "\n",
    "print( f'To fully understand {file}, you need to have followed {fk} years of formal education. ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples that have been given so far have all collected data about a single text. It is evidently possible to calculate the numbers for all the texts in a corpus, and to compare the texts on the basis of these numbers. \n",
    "\n",
    "The code below brings together all the methods that have been discussed to this point. For all the texts in the folder named 'Corpus', it creates data about the total number of tokens, the average sentence length, the average number of adjectives, adverbs and pronouns, the average number of syllables per word and result of the Flesch–Kincaid Grade Level Formula. \n",
    "\n",
    "The results are saved in a new data file named `nlp.csv`. \n",
    "\n",
    "If your texts are long or if there are many texts in your corpus, running the code may also take quite a while. The various print statements have been added to give you updates on the progress during the data creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import re\n",
    "import tdm\n",
    "from tdm import word_tokenise, flesch_kincaid, countSyllables\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk import pos_tag\n",
    "\n",
    "dir = 'Corpus'\n",
    "texts = []\n",
    "pos = []\n",
    "\n",
    "out = open( 'nlp.csv' , 'w' ,  encoding= 'utf-8' )\n",
    "\n",
    "out.write( 'title,tokens,sentences,adjectives,adverbs,persPronouns,syllables,fk\\n' )\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( 'txt$' , file ):\n",
    "        print( '\\n\\nCollecting data for ' + file + ' ... ')\n",
    "        out.write( f'{ tdm.removeExtension(file) },' )\n",
    "        \n",
    "        path = join( dir , file )\n",
    "        with open( path , encoding = 'utf-8' ) as file_handler:\n",
    "            full_text = file_handler.read()\n",
    "            sentences = sent_tokenize(full_text)\n",
    "            words = word_tokenise(full_text)\n",
    "        \n",
    "        print(\"Counting number of tokens ...  \")\n",
    "        out.write( f'{ len(words) },' )\n",
    "        print(\"Calculating number of sentences ...  \") \n",
    "        out.write( f'{ len(sentences) },' )\n",
    "        \n",
    "        print(\"Adding POS tags ...  \")\n",
    "        pos = pos_tag(words)\n",
    "\n",
    "        pos_tags = dict()\n",
    "        for p in pos:\n",
    "            pos_tags[p[1]] = pos_tags.get( p[1] , 0 ) + 1\n",
    "          \n",
    "        print(\"Counting number of adjectives ...  \")\n",
    "        \n",
    "        adjectives = [ 'JJ' , 'JJR' , 'JJS' ]\n",
    "        \n",
    "        count = 0 \n",
    "        for a in adjectives:\n",
    "            count += pos_tags[a]\n",
    "        \n",
    "        out.write( f'{count},' )\n",
    "        \n",
    "        print(\"Counting number of adverbs ...  \")\n",
    "        adverbs = [ 'RB' , 'RBR' , 'RBS' ]\n",
    "        count = 0 \n",
    "        for a in adverbs:\n",
    "            count += pos_tags[a]\n",
    "        \n",
    "        out.write( f'{count},' )\n",
    "        \n",
    "        print(\"Counting number of personal pronouns ...  \")\n",
    "        personalPronouns = [ 'PRP' ]\n",
    "        count = 0 \n",
    "        for a in personalPronouns:\n",
    "            count += pos_tags[a]\n",
    "        \n",
    "        out.write( f'{count},' )\n",
    "        \n",
    "        print(\"Calculating Flesch-Kincaid formula ...  \")\n",
    "        \n",
    "        asl = len(words) / len(sentences)\n",
    "        \n",
    "        nr_syllables = 0 \n",
    "        for word in words:\n",
    "            if re.search( r'\\w' , word):\n",
    "                nr_syllables += countSyllables( word )\n",
    "        asw = nr_syllables / len(words)\n",
    "        out.write( f'{nr_syllables},' )\n",
    "\n",
    "        fk = flesch_kincaid( asl , asw )\n",
    "        \n",
    "        out.write( f'{fk}' )\n",
    "\n",
    "\n",
    "        out.write( '\\n' )\n",
    "    \n",
    "out.close()    \n",
    "\n",
    "print(\"\\n\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV that was just created, you can create a number of visualisations illuminating the differences and the similarities between the texts in your corpus. \n",
    "\n",
    "The CSV contains the following variables:\n",
    "\n",
    "* title\n",
    "* tokens\n",
    "* sentences\n",
    "* adjectives\n",
    "* adverbs\n",
    "* persPronouns\n",
    "* syllables\n",
    "* fk\n",
    "\n",
    "These variables can all be used to make such visualisations. The code in the cell below creates a bar chart, for instance. The X-axis lists the various titles in your corpus, and the Y-axis displays the average number of words per sentence. \n",
    "\n",
    "This data about average number of words per sentence is not in the CSV originally. The CSV file contains the raw counts. To calculate the average sentence length, you need to divide the values in the column `tokens` by the values in the columnb `sentences`. The code below adds a new column named `sentenceLength`. \n",
    "\n",
    "The variables `x_axis` and `y_axis` specify the values to be used in this visualisation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv( 'nlp.csv' )\n",
    "\n",
    "\n",
    "\n",
    "data['sentenceLength'] = data['tokens'] / data['sentences'] \n",
    "\n",
    "y_axis = 'sentenceLength'\n",
    "x_axis = 'title'\n",
    "\n",
    "fig = plt.figure( figsize=( 12 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "bar_width = 0.6\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( list(data[x_axis]) , list(data[y_axis]) , width = bar_width, alpha = opacity , color = '#781926')\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Texts' , fontsize= 12)\n",
    "ax.set_ylabel('Average sentence length' , fontsize = 12 )\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#plt.savefig('barchart.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below visualises two sets of values  simultaneously in a scatter plot. \n",
    "\n",
    "As was mentioned, the CSV file contains raw counts. To ensure that the texts can be compared fairly, we need to normise these raw counts. We can do this by dividing the absolute number by the number of tokens. The numbers that result from such a division represent proportions. If we divide the total number of adverbs by the total number of words, this is similar to calculating the percentage of adverbs in the full text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "data = pd.read_csv( 'nlp.csv' )\n",
    "\n",
    "data['adverbs_normalised'] = data['adverbs'] / data['tokens'] \n",
    "data['adjectives_normalised'] = data['adjectives'] / data['tokens'] \n",
    "\n",
    "x_axis = 'adverbs_normalised' \n",
    "y_axis = 'adjectives_normalised' \n",
    "\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize = ( 8,8 ))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "ax.scatter( list(data[x_axis])  , list(data[y_axis]) , alpha=0.8,  s=90 )\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    plt.text( row[x_axis] , row[y_axis] , row['title'] , fontsize=14)\n",
    "    \n",
    "\n",
    "ax.set_xlabel( 'Average number of adverbs' , fontsize = 16 )\n",
    "ax.set_ylabel( 'Average number of adjectives'  , fontsize = 16 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for other languages\n",
    "\n",
    "Many of the default methods in the `NLTK` library, such as `pos_tag`, were trained on texts in modern English. If you want to work with other languages, you need to change the model underlying these methods. \n",
    "\n",
    "For texts in the Dutch langauge, for instance, you can make use of the model trained on the [Alpino](https://www.let.rug.nl/~vannoord/trees/) corpus. You can do this as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('alpino')\n",
    "\n",
    "from nltk.corpus import alpino as alp\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "training_corpus = alp.tagged_sents()\n",
    "unitagger = UnigramTagger(training_corpus)\n",
    "bitagger = BigramTagger(training_corpus, backoff=unitagger)\n",
    "pos_tag = bitagger.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import word_tokenise\n",
    "\n",
    "sentence = 'Het was nog donker, toen in de vroege morgen van de tweeëntwintigste december 1946 in onze stad, op de eerste verdieping van het huis Schilderskade 66, de held van deze geschiedenis, Frits van Egters, ontwaakte.'\n",
    "\n",
    "words = word_tokenise(sentence)\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is t make use of another NLP library named [spaCy](https://spacy.io/). This NLP library offers support for [a wide range of lanaguges](https://spacy.io/usage/models). These langauge models can all be downloaded from the spaCy website. \n",
    "\n",
    "library is not part of the Anaconda distribution of Python, so if you have never worked with spaCy before, the library needs to be installed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of [models for the Dutch language](https://spacy.io/models/nl), for instance. You can use the code below to download the model named `nl_core_news_lg`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python3 -m spacy download nl_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been downloaded, it needs to be loaded into your code, so that you can start to work with it. The `load()` method in `spaCy` creates a new object which can be used to add linguistic and semantic annotations. in the cell below, this is object is given the name nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newly created `nlp` object can given a string as input. Its output will be a tagged text giving information about a number of grammatical and morphological aspects of this string, including the parts of speech, the sentence boundaries and the lemmatised form. \n",
    "\n",
    "In the code below, the output of the `nlp()` method is assigned to a variable named `tagged_text`. The annotations can be accessed by naviagting through the string word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "tagged_text = nlp(\"'Het is gezien', mompelde hij, 'het is niet onopgemerkt gebleven.''\")\n",
    "\n",
    "for w in tagged_text:\n",
    "    print( f'{w.text} (pos: {w.pos_} ; lemma: {w.lemma_})' )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to use `spaCy` to produce data about the number of words, sentences, adverbs, pronouns, adjectives and conjunctions for all the texts in a folder named 'Corpus'. The process of adding linguistic annotations may demand some time, unfortunately. The code below used the `timeit` library to track how long this process actually takes. With longer texts, this process may take more than a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from tdm import removeExtension\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "out = open( 'nlp.csv' , 'w' ,  encoding = 'utf-8')\n",
    "\n",
    "# CSV header\n",
    "out.write( 'title,tokens,sentences,' )\n",
    "out.write(  'adverbs,verbs,pronouns,nouns,adjectives,conjunctions,aux-verbs\\n')\n",
    "\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( r'.txt$' , file ):\n",
    "        print( f'Adding annotations for {file} ... ')\n",
    "        out.write( removeExtension(file) )\n",
    "        path = os.path.join(dir,text)\n",
    "        with open(path) as file_handler:\n",
    "            full_text = file_handler.read()\n",
    "        start_time = timeit.default_timer()\n",
    "        annotated_text = nlp(full_text)\n",
    "        end_time = timeit.default_timer()\n",
    "        print( f'Done! The annotation process took {end_time-start_time} seconds.')\n",
    "        nr_words = len(annotated_text) \n",
    "        nr_sentences = len(list( annotated_text.sents ))\n",
    "        out.write( f',{nr_words},{nr_sentences}')\n",
    "\n",
    "        for w in annotated_text:\n",
    "            pos[ w.pos_ ] = pos.get( w.pos_ , 0 ) + 1\n",
    "            \n",
    "        out.write( f\",{pos.get('ADV',0)}\" )\n",
    "        out.write( f\",{pos.get('VERB',0)}\" )\n",
    "        out.write( f\",{pos.get('PRON',0)}\" )\n",
    "        out.write( f\",{pos.get('NOUN',0)}\" )\n",
    "        out.write( f\",{pos.get('ADJ',0)}\" )\n",
    "        out.write( f\",{pos.get('SCONJ',0)+pos.get('CCONJ',0)}\" )\n",
    "        out.write( f\",{pos.get('AUX',0)}\" )\n",
    "        out.write( '\\n')\n",
    "            \n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
