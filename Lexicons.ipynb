{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicons\n",
    "\n",
    "Frequency lists can show us the words that occur in a text, but they don't necessary offer insights on the broader topics that are discussed. If we want to uncover the semantics of texts, we can make use of word lists which map the textâ€™s tokens to pre-defined semantic categories. We can make a list of words which all have to do with 'religion', for instance. Next, by counting the numbers of times the text uses one of the tokens on the word, we can form an impression of the importance of this particualr topic within out text.  \n",
    "\n",
    "Examples of applications in which this principle is implemented include\n",
    "the [Harvard General Inquirer (HGI)](http://www.wjh.harvard.edu/~inquirer/homecat.htm), [the Linguistic Inquiry and Word Count (LIWC)\n",
    "tool](http://liwc.wpengine.com/)  and the [UCREL Semantic Analysis System (USAS)](http://ucrel.lancs.ac.uk/usas/). The programmers responsible for the *Harvard General Inquirer*, for example, have defined 182 semantic categories, and they have compiled long list of words pertaining to these categories.  Such word lists are usually referred to as lexicons. \n",
    "\n",
    "To let you work with the possibilities of semantic tagging, a number of the lexicons that have been made available have been downloaded and merged. Next to the lexicons developed for the HGI and USAS, the word lists created for this course also include terms taken from lists compiled by [Bing Liu](https://www.cs.uic.edu/~liub/) and by the project team that worked on the [Multi-Perspective Question Answering (MPQA) tool](http://mpqa.cs.pitt.edu/). \n",
    "\n",
    "The merged semantic lexicons can be found here: \n",
    "https://github.com/peterverhaar/semanic-tagging/tree/main/Lexicons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, the lexicon files that are available are all mentioned in the list named `lexicon_files`. \n",
    "\n",
    "The code downloads all of these lexicon files. The terms that are mentioned in these files are saved in a list. Next, these lists are saved in a dictionary named `lexicons`. The keys of this dictionary are the names of the topics. These topic names are are derived from the file names that are given in `lexicon_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "baseUrl = 'https://raw.githubusercontent.com/peterverhaar/semanic-tagging/main/Lexicons/'\n",
    "lexicon_files = [  'Academic.txt' , 'Economics.txt' ,  'Legal.txt' , 'Military.txt' , 'Movement.txt' , 'Pain.txt' , 'Passive.txt' , 'Pleasure.txt' , 'Politics.txt' , 'Power.txt' , 'Religion.txt' , 'Space.txt' , 'Time.txt' , 'Transportation.txt' , 'Vice.txt' , 'Weather.txt' , 'workAndEmployment.txt' ]\n",
    "\n",
    "lexicons = dict()\n",
    "\n",
    "\n",
    "for l in lexicon_files:\n",
    "    topic = l[ : l.rindex('.') ]\n",
    "    response = requests.get( baseUrl + l)\n",
    "    words = []\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        lines = re.split( r'\\n' , response.text )\n",
    "        for l in lines: \n",
    "            if re.search( r'\\w' , l ):\n",
    "                words.append(l.strip())\n",
    "    else:\n",
    "        print('Cannot download lexicon file!')\n",
    "    lexicons[topic] = words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code below to count the number of occurrences of the words in these various lexicons within the texts of your corpus. The code searches in lemmatised versions of all the corpus texts. \n",
    "\n",
    "The result (consisting of counts for all the texts in your corpus) is stored in a file named 'lexicon.csv'.\n",
    "\n",
    "If your texts are long, or if the corpus contains many texts, running the code make take quite a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatising SonsandLovers.txt ...\n",
      "Performing semantic tagging for SonsandLovers.txt ...\n",
      "Academic ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-67c1dc7934fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mcountOccurrences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m',{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count' is not defined"
     ]
    }
   ],
   "source": [
    "import tdm\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "\n",
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write( 'title' )\n",
    "for l in lexicons:\n",
    "    csv.write( f',{l.lower().strip() }' )\n",
    "csv.write('\\n')\n",
    "\n",
    "dir = 'Corpus'\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( r'\\.txt$' , file ):\n",
    "        csv.write( tdm.removeExtension( file ) )\n",
    "        path = join( dir, file )\n",
    "        \n",
    "        print( '\\nLemmatising {} ...'.format( file ) )\n",
    "        novel = tdm.text(path)\n",
    "        lemmatised = novel.lemmatise()\n",
    "        \n",
    "        print( 'Performing semantic tagging for {} ...'.format( file ) )\n",
    "        \n",
    "        words = tdm.word_tokenise(lemmatised)\n",
    "        freq = dict()\n",
    "        for w in words:\n",
    "            freq[w] = freq.get(w,0)+1\n",
    "        tokens = len(lemmatised)\n",
    "        \n",
    "        for l in lexicons:\n",
    "            print(f'{l} ...')    \n",
    "            \n",
    "            countOccurrences = 0\n",
    "            for word in l:\n",
    "                countOccurrences += freq.get(l,0)\n",
    "            \n",
    "            csv.write( ',{}'.format( count / tokens ) )\n",
    "        csv.write('\\n')\n",
    "        \n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, the counts that have made for the terms from the various lexicons can be visualised as a bar chart. As the value of the variable named `y`, you need to type in the name of the lexicon, without the .txt extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('lexicon.csv')\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "x = 'title'\n",
    "y = 'politics'\n",
    "\n",
    "\n",
    "bar_width = 0.45\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( df[x] , df[y] , width = bar_width, alpha = opacity , color = '#23a145')\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Categories' , fontsize= 12)\n",
    "ax.set_ylabel('Mean values' , fontsize = 12 )\n",
    "ax.set_title( y.title() , fontsize=20 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below, finally, creates a bar chart which visualises the counts for three lexicons simultaneously. The counts to be shown need to be specified in the list named `lexiconsInChart`. This bar chart enables you to compare the values collected for the three semantic domains you have listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "data = pd.read_csv( 'lexicon.csv' )\n",
    "\n",
    "\n",
    "x_axis = 'time' \n",
    "y_axis = 'religion' \n",
    "\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize = ( 8,8 ))\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "ax.scatter( list(data[x_axis])  , list(data[y_axis]) , alpha=0.8,  s=95 )\n",
    "\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    plt.text( row[x_axis] , row[y_axis] , row['title'] , fontsize=14)\n",
    "    \n",
    "\n",
    "ax.set_xlabel( x_axis.title() , fontsize = 16 )\n",
    "ax.set_ylabel( y_axis.title()  , fontsize = 16 )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
