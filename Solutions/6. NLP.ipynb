{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1\n",
    "\n",
    "Create a list containing the unique adjectives that are occur in *Pride and Prejudice*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from os.path import join\n",
    "\n",
    "path = join('Corpus','PrideAndPrejudice.txt') \n",
    "with open(path, encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "\n",
    "adjectives = []\n",
    "\n",
    "## this code reuses variable full_text created in exercise 6.2\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    for i in tags:\n",
    "        if i[1].startswith('JJ'):\n",
    "\n",
    "            adjectives.append(i[0])\n",
    "\n",
    "adjectives = remove_punctuation(adjectives)\n",
    "\n",
    "for adj in sorted( set(adjectives) ):\n",
    "    print(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2\n",
    "\n",
    "Stephen King is [reputed to have said](https://www.goodreads.com/quotes/430289-i-believe-the-road-to-hell-is-paved-with-adverbs) that â€œthe road to hell is paved with adverbs\", and many style guides similarly give writers the advice to avoid adverbs, especially those ending in '-ly'. \n",
    "\n",
    "Can you calculate, for each text in the corpus, the number of adverb ending in '-ly', measured as a percentage of the total number of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from os.path import join\n",
    "from tdmh import *\n",
    "\n",
    "dir = 'Corpus'\n",
    "scores = dict()\n",
    "\n",
    "for text in os.listdir(dir):\n",
    "\n",
    "    path = join('Corpus', text ) \n",
    "    with open(path, encoding = 'utf-8') as fh:\n",
    "        full_text = fh.read()\n",
    "\n",
    "    nr_words = 0\n",
    "    ly_adverbs = []\n",
    "\n",
    "    ## this code reuses variable full_text created in exercise 6.2\n",
    "    sentences = sent_tokenize(full_text)\n",
    "\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        nr_words += len(words)\n",
    "        \n",
    "        tags = nltk.pos_tag(words)\n",
    "        for i in tags:\n",
    "            if i[1].startswith('RB') and re.search( r'ly$' ,  i[0]):\n",
    "                ly_adverbs.append(i[0])\n",
    "    \n",
    "    ratio = len(ly_adverbs) / nr_words\n",
    "    scores[text] = ratio\n",
    "    \n",
    "for text in sortedByValue(scores , ascending = False):\n",
    "    print( f'{text}: {round( scores[text] *100 , 3) }% of the words are adverbs ending in -ly' )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execise 6.3\n",
    "\n",
    "Which text in the corpus has the highest number of modal verbs? The Penn Treebank code for 'modal auxialiaries' is MD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from os.path import join\n",
    "from tdmh import *\n",
    "\n",
    "dir = 'Corpus'\n",
    "scores = dict()\n",
    "\n",
    "for text in os.listdir(dir):\n",
    "\n",
    "    path = join('Corpus', text ) \n",
    "    with open(path, encoding = 'utf-8') as fh:\n",
    "        full_text = fh.read()\n",
    "\n",
    "    nr_words = 0\n",
    "    modal = []\n",
    "\n",
    "    ## this code reuses variable full_text created in exercise 6.2\n",
    "    sentences = sent_tokenize(full_text)\n",
    "\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        nr_words += len(words)\n",
    "        \n",
    "        tags = nltk.pos_tag(words)\n",
    "        for i in tags:\n",
    "            if i[1].startswith('MD'):\n",
    "                modal.append(i[0])\n",
    "    \n",
    "    ratio = len(modal) / nr_words\n",
    "    scores[text] = ratio\n",
    "    \n",
    "for text in sortedByValue(scores , ascending = False):\n",
    "    print( f'{text}: {round( scores[text] *100 , 3) }% of the words are modal verbs.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4\n",
    "\n",
    "Extract all the sentences from *HeartOfDarkness.txt* that contain an adjective in the superlative form.  Write these sentences into a file named 'sentences.txt'. The code for the words in these category is 'JJS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join \n",
    "\n",
    "path = join( 'Corpus' , 'HeartOfDarkness.txt' )\n",
    "with open( path , encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "out = open('sentences.txt' , 'w' , encoding = 'utf-8')\n",
    "\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    for i in tags:\n",
    "        if i[1].startswith('JJS'):\n",
    "            out.write(f'{s}\\n')\n",
    "            break\n",
    "\n",
    "out.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.5\n",
    "\n",
    "Extract all the sentences from *ARoomWithaView.txt* containing a form of the verb 'to see', in all tenses and conjugations and excepting the infitive form. In other words, extract sentences containing forms such as 'seen', 'saw' or 'seeing', but not 'see'. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tdm import *\n",
    "\n",
    "\n",
    "from os.path import join \n",
    "\n",
    "path = join( 'Corpus' , 'ARoomWithaView.txt' )\n",
    "with open( path , encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "\n",
    "    \n",
    "lemmatiser = WordNetLemmatizer()\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    \n",
    "    for i,w in enumerate(words):\n",
    "\n",
    "        posTag = ptb_to_wordnet( pos[i][1] )\n",
    "\n",
    "        if re.search( r'\\w+' , posTag , re.IGNORECASE ):\n",
    "            lemma = lemmatiser.lemmatize( words[i] , posTag )\n",
    "            #print(lemma)\n",
    "            if lemma == 'see' and words[i] != 'see':\n",
    "                print( f'{sent}\\n' )\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.6\n",
    "\n",
    "From *HeartOfDarkness.txt* , extract all sentnces containing the following combinations of categories: \n",
    "\n",
    "* Article - adverb - adjective - noun \n",
    "\n",
    "These categorties can be asigned the following codes:\n",
    "\n",
    "* Article: DT\n",
    "* Adverb: RB, RBR or RBS\n",
    "* Adjective: JJ, JJR or JJS\n",
    "* Noun: NN, NNP, NNPS or NNS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from tdm import *\n",
    "\n",
    "\n",
    "from os.path import join \n",
    "\n",
    "path = join( 'Corpus' , 'ARoomWithaView.txt' )\n",
    "with open( path , encoding = 'utf-8') as fh:\n",
    "    full_text = fh.read()\n",
    "    \n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    words = remove_punctuation(words)\n",
    "    pos = pos_tag(words)\n",
    "    \n",
    "    tagged_sentence = ''\n",
    "\n",
    "    for p in pos:\n",
    "        tagged_sentence += p[1] + ' '\n",
    "\n",
    "    if re.search( r'DT RB\\w? JJ\\w? NN\\w?' , tagged_sentence):\n",
    "        print(sent)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
