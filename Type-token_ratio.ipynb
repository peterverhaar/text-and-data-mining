{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type-token ratio\n",
    "\n",
    "As was discussed in one of the previous notebooks, the individual words that are found in a text are referred to as “tokens”, and the unique words are called “types”. Frequency lists count occurrences of types. \n",
    "\n",
    "The ratio between the number of types and the number of tokens can, under certain conditions, offer useful information about texts as well. The type-token ratio is calculated by dividing the number of types by the number of tokens. This division obviously results a number in between 0 and 1. This number gives an indication of the lexical diversity: the capacity of the author to vary the vocabulary. \n",
    "\n",
    "If the type-token ratio is high, this indicates that the author uses many unique words and that the text contains very little lexical repetition. If, by contrast, the type-token ratio is low, this implies that the same words recur frequently, and that there is a low level of lexical diversity. \n",
    "\n",
    "The code below demonstrates how the type-token ratio may be calculated, using the `word_tokenise()` function from the `tdm` module that was developed for this tutorial.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import re\n",
    "import tdm\n",
    "\n",
    "\n",
    "dir = 'Corpus'\n",
    "text = 'ARoomWithAView.txt'\n",
    "path = join( dir, text )\n",
    "\n",
    "with open( path , encoding = 'utf-8' ) as file:\n",
    "    full_text = file.read()\n",
    "\n",
    "words = tdm.word_tokenise(full_text)\n",
    "\n",
    "tokens = len(words)\n",
    "unique_words = set(words)\n",
    "types = len(unique_words)\n",
    "\n",
    "ttr = types / tokens\n",
    "\n",
    "print( f'Types: {types}' )\n",
    "print( f'Tokens: {tokens}' )\n",
    "print( f'Type-token ratio: {ttr}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above makes use of the function `set()`. It can be used to convert a list into a set. A set is default data structure in Python, which is very similar to a list. An important difference, however, is that, while a list may contain the same item multiple times, a set can only contain unique items. A list also stores the items in a specific order, while a set is **un**ordered. The `set()` function can be used very effectively to deduplicate a Python list.\n",
    "\n",
    "The code that is given above can also be applied to all the texts in a corpus. This may enable us to compare the lexical diversity of all the texts that are studied. \n",
    "\n",
    "The code below defines a function named `ttr()` with takes a reference to a text as a parameter. It opens the file, accesses the full text and calculates the type-token ratio using the same code that was explained above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from os.path import join\n",
    "import tdm\n",
    "\n",
    "def removeExtension(text):\n",
    "    new_text = re.sub( '\\.txt' , '' , text )\n",
    "    return new_text\n",
    "    \n",
    "\n",
    "def ttr(full_text):\n",
    "\n",
    "    words = tdm.word_tokenise(full_text)\n",
    "    \n",
    "    for w in words: \n",
    "        if not( re.search( r'\\w' , w )):\n",
    "            words.remove(w)\n",
    "\n",
    "    tokens = len(words)\n",
    "    unique_words = set(words)\n",
    "    types = len(unique_words)\n",
    "\n",
    "    return types / tokens\n",
    "    \n",
    "    \n",
    "dir = 'Corpus'    \n",
    "for text in os.listdir(dir):\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        path = join( dir , text) \n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            full_text = full_text.lower()\n",
    "        \n",
    "        print( f'{ removeExtension(text) }: {ttr(full_text)} ' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you work with the type-token ratios, it is important to realise that\n",
    "the result of such calculations tend to vary along with the total length of the text. In a relatively short text, it is easier for an author to continue to introduce new words as the text progresses. When texts become much longer, however, the chances that words will be repeated also increase accordingly. Shorter texts generally have much higher type-token ratios. \n",
    "\n",
    "One solution can be to ensure that all the texts are of the same lengths before calculating the type token ratios. We can do this by firstly calculating the lenght (i.e. the total number of words) of the **shortest text in the corpus**. Next, we can artifically harmonise the lengths of all the texts by creating substrings of the longer texts. These substrings should have exactly the same number of words as the shortest text in the corpus. The code below illustrates this principle.  \n",
    "\n",
    "The code below reuses the function `ttr()` defined in the code above, so make sure that you have run the cell above before you attempt to run the cell below. You can also choose to run all cells of this notebook by choosing \"Cell\" > \"Run all\" form the menu at the top.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "texts = []\n",
    "min_tokens = 0 \n",
    "import tdm\n",
    "\n",
    "for text in os.listdir(dir):\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        texts.append(text)\n",
    "        path = join( dir , text) \n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            words = tdm.word_tokenise(full_text)\n",
    "            tokens = len(words)\n",
    "            print( f'{text} contains {tokens} words.' )\n",
    "            if min_tokens == 0:\n",
    "                min_tokens = tokens\n",
    "            elif tokens < min_tokens:\n",
    "                min_tokens = tokens\n",
    "                \n",
    "print( f'\\nShortest text has {min_tokens} words.' )\n",
    "\n",
    "ttr_scores = dict()\n",
    "                \n",
    "            \n",
    "for text in texts:\n",
    "    if re.search( r'\\.txt' , text ):\n",
    "        path = join( dir , text) \n",
    "        print( f'Calculating the TTR of {path}' )\n",
    "        with open( path , encoding = 'utf-8' ) as file:\n",
    "            full_text = file.read()\n",
    "            full_text = full_text.lower()\n",
    "            full_text = full_text[ 0 : min_tokens]\n",
    "        \n",
    "        print( f'{ tdm.removeExtension(text) }: {ttr(full_text)} ' )\n",
    "        ttr_scores[ tdm.removeExtension(text) ] = ttr(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final line of the code above populates a dictionary named `ttr_scores`. The titles of the texts in the corpus serve as keys. The type-token ratios that are calculated are stored as the values. \n",
    "\n",
    "This dictionary can be used to visualise the type-token ratios in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x_axis = list(ttr_scores.keys())\n",
    "y_axis = list(ttr_scores.values())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fig = plt.figure( figsize=( 7 ,6 ) )\n",
    "ax = plt.axes()\n",
    "\n",
    "\n",
    "bar_width = 0.6\n",
    "opacity = 0.8\n",
    "\n",
    "ax.bar( x_axis , y_axis , width = bar_width, alpha = opacity , color = '#781926')\n",
    "\n",
    "plt.xticks(rotation= 90)\n",
    "\n",
    "ax.set_xlabel('Texts' , fontsize= 12)\n",
    "ax.set_ylabel('Type-token ratio' , fontsize = 12 )\n",
    "ax.set_title( 'Lexical diversity' , fontsize=28 )\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
