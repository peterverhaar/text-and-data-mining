{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diction\n",
    "\n",
    "The term 'diction' generally refers to the stylistic choices that are made by an author while writing a text. A study of the diction of an author may concentrate, among other thing, on the words that are chosen. In stylometric research, it can be interesting to study the words that are characteristic of a given author, and to examine how the words that are chosen differ from the words chosen by other authors. \n",
    "\n",
    "One of statistical methods that can be used to find such distinctive words is *Dunning's log likelihood*. In short, it analyses the distinctiveness of word in one set of texts compared to the texts in a reference corpus, by calculating probabilities based on word frequencies. A good explanation of the fomula can be found on the [wordHoard](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html#loglike) website. \n",
    "\n",
    "This notebook explains how to compare the diction used within two distinct corpora using the *Dunning's log likelihood*. \n",
    "\n",
    "## Defing corpora\n",
    "\n",
    "As a first step, we need to define the two corpora whose words need to be compared. In this notebook, the words in two early 20th century novels will be compared to the words used in two early 19th century novels. \n",
    "\n",
    "The code below defines two lists named `corpus1` and `corpus2`. The files that are mentioned can all be used in a folder named `Corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "\n",
    "corpus1 = [ 'ARoomWithAView.txt' , 'SonsandLovers.txt' ]\n",
    "corpus2 = [ 'Ivanhoe.txt' , 'TreasureIsland.txt' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating frequencies\n",
    "\n",
    "The code in the cell below reads in the full text of the texts that are listed in `corpus1`. Using the function `findWordsFrequencies`, it finds the freuquencies of the tokens in these texts. These frequencies are added to a dictionary named `freq1`, using the method `update()`. \n",
    "\n",
    "After this, the code does the same for the texts in `corpus2`. The word frequencies are placed in a dictionary named `freq2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdm \n",
    "from os.path import join\n",
    "\n",
    "def findWordsFrequencies( file ):\n",
    "    freq = dict()\n",
    "    with open( file ) as file_handler:\n",
    "        full_text = file_handler.read()\n",
    "    \n",
    "    words = tdm.word_tokenise( full_text )\n",
    "    for w in words:\n",
    "        freq[w] = freq.get(w,0) +1\n",
    "    return freq\n",
    "\n",
    "freq1 = dict()\n",
    "\n",
    "for text in corpus1:\n",
    "    print(text)\n",
    "    freq_text = findWordsFrequencies( join(dir,text) )\n",
    "    freq1.update(freq_text) \n",
    "    \n",
    "freq2 = dict()\n",
    "    \n",
    "for text in corpus2:\n",
    "    print(text)\n",
    "    freq_text = findWordsFrequencies( join(dir,text) )\n",
    "    freq2.update(freq_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using the frequencies that have been calculated in this way, the Dunning log likelihood scores are calculated for all of the words that occur both in `corpus1` and `corpus2`. The actual calculation takes place in a method named `log_likelihood()`. The scores that are calculated are all stored in a dictionary named `ll_scores`\n",
    "\n",
    "The formula that is implemented in the `log_likelihood` returns a number which can either be positive or negative. A postive score indicates that there is a high probability that the word will be used in the first corpus. and a relatively low probability that the the word occurs in the  second corpus. The tokens that are assigned the highest scores, in other words, are also most distincive of the first corpus. \n",
    "\n",
    "The code below lists the words that are given a positive log likelihood score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdm\n",
    "import math\n",
    "\n",
    "def log_likelihood( word_count1 , word_count2, total1 , total_2 ):\n",
    "\n",
    "    a = word_count1\n",
    "    b = word_count2\n",
    "    c = total1\n",
    "    d = total2\n",
    " \n",
    "    perc1 = (a/c)*100\n",
    "    perc2 = (b/d)*100\n",
    "    polarity = perc1 - perc2\n",
    " \n",
    "    E1 = c*(a+b)/(c+d)\n",
    "    E2 = d*(a+b)/(c+d)\n",
    "    \n",
    "    ln1 = math.log(a/E1)\n",
    "    ln2 = math.log(b/E2)\n",
    "    G2 = 2*((a* ln1) + (b* ln2))\n",
    "    \n",
    "    #if polarity < 0:\n",
    "    #    G2 = -G2\n",
    "    if a * math.log(a / E1) < 0:\n",
    "        G2 = -G2\n",
    "\n",
    "    return G2\n",
    "\n",
    "\n",
    "\n",
    "ll_scores = dict()\n",
    "\n",
    "total1 = 0\n",
    "total2 = 0\n",
    "\n",
    "for word1 in freq1:\n",
    "    total1 += freq1[word1]\n",
    "for word2 in freq2:\n",
    "    total2 += freq2[word2]\n",
    "\n",
    "for word in freq1:\n",
    "    if word in freq2:\n",
    "\n",
    "        ll_score = log_likelihood( freq1[word] , freq2[word] , total1 , total2 )\n",
    "        ll_scores[word] = ll_score\n",
    "        \n",
    "for word in reversed( tdm.sortedByValue(ll_scores) ):\n",
    "    #print( ll_scores['lowly'] )\n",
    "    print( word , ll_scores[word] )\n",
    "    if ll_scores[word] < 0:\n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with negative log likelihood scores are more likely to appear in the reference corpus (i.e. the second corpus) than in the first corpus. \n",
    "\n",
    "The code below lists the words with the lowest scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tdm.sortedByValue(ll_scores) :\n",
    "    print( word , ll_scores[word] )\n",
    "    if ll_scores[word] > 0:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dunning, Ted, 'Accurate Methods for the Statistics of Surprise and Coincidence', in *Computational Linguistics*, 19:1 (1993).\n",
    "* Rayson, P. and Garside, R., 'Comparing corpora using frequency profiling', in *Proceedings of the workshop on Comparing Corpora, held in conjunction with the 38th annual meeting of the Association for Computational Linguistics (ACL 2000)* (2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
