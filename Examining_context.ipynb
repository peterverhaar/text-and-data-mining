{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the context of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Concordances\n",
    "\n",
    "To examine *how* words are used in a text, it can be useful to create a concordance. In a concordance, all the occurrences of a given search term are shown in combination with words that occur before and after this term. Such resources are sometimes referred to as *keyword in context (KWIC)* lists. \n",
    "\n",
    "The code below defines a method named `concordance()` which can help you to examine the contexts of words. To work with it, you need to supply three parameters: (1) the text that you want to analyse (i.e. its filename); (2) a regular expression representing the word(s) whose contexts you want to examine; and (3) a number that specifies the extent of the context. The number that is given as the third parameter determines the number of characters that will be shown before and after the search result.\n",
    "\n",
    "The last few lines also demonstrate how you can use this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concordance( file,search_term,window ): \n",
    "    \n",
    "    concordance = []\n",
    "    regex = r'\\b{}\\b'.format( search_term )\n",
    "    \n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "\n",
    "        if re.search( regex , line , re.IGNORECASE ):\n",
    "            extract = ''\n",
    "            position = re.search( regex , line , re.IGNORECASE ).start()\n",
    "            start = position - len( search_term ) - window ;\n",
    "            fragmentLength = start + 2 * window  + 2 * len( search_term )\n",
    "            if fragmentLength > len( line ):\n",
    "                fragmentLength = len( line )\n",
    "\n",
    "            if start < 0:\n",
    "\n",
    "                whiteSpace = ''\n",
    "                i = 0\n",
    "                while i < abs(start):\n",
    "                    whiteSpace += ' '\n",
    "                    i += 1\n",
    "                extract = whiteSpace + line[ 0 : fragmentLength ]\n",
    "            else:\n",
    "                extract = line[ start : fragmentLength ]\n",
    "\n",
    "            if re.search( '\\w' , extract ) and re.search( regex , extract , re.IGNORECASE ):\n",
    "                concordance.append( extract )\n",
    "                \n",
    "    return concordance\n",
    "\n",
    "    \n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "c = concordance( join( dir,file ) , 'love' , 20 )\n",
    "\n",
    "for line in c:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. Such analyses give an impression of the semantic contexts of these term. A collocation analysis can be performed using the `collocation()` method. The parameters are the same as those of the `concordance()` method. This function returns a dictionary listing all the words found near the search term that is provided, together with the frequencies of these words. \n",
    "\n",
    "The function `removeStopWords()` can also be useful in this context. It removes the stopwords a given dictionary. This function makes use of the list of stopwords defined in the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def collocation( file , search_term , distance ):\n",
    "\n",
    "    freq = dict()\n",
    "    \n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "    regex = r'\\b{}\\b'.format( search_term )\n",
    "\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        words = word_tokenise( line )\n",
    "        i = 0\n",
    "        for w in words:\n",
    "            if re.search( regex , w , re.IGNORECASE ):\n",
    "\n",
    "                match = re.search( regex , w , re.IGNORECASE )\n",
    "                searchTerm = match.group(0)\n",
    "\n",
    "                for x in range( i - distance , i + distance ):\n",
    "                    if x >= 0 and x < len(words) and search_term != words[x]:\n",
    "                        if len(words[x]) > 0:\n",
    "                            freq[ words[x] ] = freq.get( words[x] , 0 ) + 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "def removeStopWords( freq ):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    filtered = dict()\n",
    "\n",
    "    for w in freq:\n",
    "            if w not in stopwords_list:\n",
    "                filtered[w] = freq[w]\n",
    "\n",
    "    return filtered\n",
    "\n",
    "            \n",
    "def sortedByValue( dict ):\n",
    "    return sorted( dict , key=lambda x: dict[x])\n",
    "\n",
    "    \n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "c = collocation( join( dir,file ) , 'florence' , 10 )\n",
    "c = removeStopWords(c)\n",
    "\n",
    "            \n",
    "count = 0 \n",
    "max = 30 \n",
    "\n",
    "for word in reversed( sortedByValue(c) ):\n",
    "    count += 1\n",
    "    print( f'{ count }. { word } => { freq[word]}' )\n",
    "    if count == max:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
