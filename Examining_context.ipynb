{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the context of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Concordances\n",
    "\n",
    "To examine *how* words are used in a text, it can be useful to create a concordance. In a concordance, all the occurrences of a given search term are shown in combination with words that occur before and after this term. Such resources are sometimes referred to as *keyword in context* lists (KWIC). \n",
    "\n",
    "The code below defines a method named `concordance()` which can help you to examine the contexts of words. To work with it, you need to supply three parameters: (1) the text that you want to analyse (i.e. its filename); (2) a regular expression representing the word(s) whose contexts you want to examine; and (3) a number that specifies the extent of the context. The number that is given as the third parameter determines the number of characters that will be shown before and after the search result.\n",
    "\n",
    "The last few lines also demonstrate how you can use this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import re\n",
    "\n",
    "def concordance_char( file,search_term,window ): \n",
    "    \n",
    "    concordance = []\n",
    "    regex = r'\\b{}\\b'.format( search_term )\n",
    "    \n",
    "    text = open( file , encoding = 'utf-8' )\n",
    "\n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "\n",
    "        if re.search( regex , line , re.IGNORECASE ):\n",
    "            extract = ''\n",
    "            position = re.search( regex , line , re.IGNORECASE ).start()\n",
    "            start = position - len( search_term ) - window ;\n",
    "            fragmentLength = start + 2 * window  + 2 * len( search_term )\n",
    "            if fragmentLength > len( line ):\n",
    "                fragmentLength = len( line )\n",
    "\n",
    "            if start < 0:\n",
    "\n",
    "                whiteSpace = ''\n",
    "                i = 0\n",
    "                while i < abs(start):\n",
    "                    whiteSpace += ' '\n",
    "                    i += 1\n",
    "                extract = whiteSpace + line[ 0 : fragmentLength ]\n",
    "            else:\n",
    "                extract = line[ start : fragmentLength ]\n",
    "\n",
    "            if re.search( '\\w' , extract ) and re.search( regex , extract , re.IGNORECASE ):\n",
    "                concordance.append( extract )\n",
    "                \n",
    "    return concordance\n",
    "\n",
    "    \n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "c = concordance_char( join( dir,file ) , 'florence' , 20 )\n",
    "\n",
    "for line in c:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `concordance_char()` that was defined above the 'window', i.e. the length of the text fragments used before and after the search term, is measured in characters. this is why the suffix '_char' is in the name of the function. When you use the number 20 as a third parameter, for instance, you will see the 20 characters that are used before and after the various instances of the search term. With such fixed number of characters, the search term is on the same position for each line, resulting in a keyword-in-context list with a nice and orderly appearance.\n",
    "\n",
    "The downside of this approach is that the various lines may contain incomplete words. The code simply removes all characters preceding or following the twentieth character. \n",
    "\n",
    "The method named `concordance`, as defined in the `tdm` module, works with words rather with characters. When you supply the number 10 as the value for the parameter defining the window, you will receive all occurrences of the search term, together with 10 the words that are used before and after the this word. \n",
    "\n",
    "The `tdm` module has defined a Class named `text`. You can initialise this Class using the `text()`. During the initialisation, you need to provide the name of the text file. If the text file is not in the same directory as this notebook, you need to indicate the path to this file. This `text()` method reads in the context of the full use, using the `open()` method. If you want, you can obviously inspect the code used to make this initialisation method by openening the module, named `tdm.py`. \n",
    "\n",
    "Once the `text` Class has been initialised using a speicifc text file, you can make use of its `concordance()` method. Here, you need to supply the search term as the first parameter and the size of the window as the second parameter. The method returns a list containing all the lines of the concordance. You can navigate across all these lines using a for-loop. In the code below, all of these lines are printed on the screen, but they are also exported to a new file named 'concordance.txt'. \n",
    "\n",
    "the `tdm` module also contains the method `concordance_char()`, that you worked with earlier. To create a concordance in which the lengths of the text fragments are specified in characters, you simply need to replace the `concordance()` method with the `concordance_char()` in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdm\n",
    "from os.path import join\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "path = join( dir , 'ARoomWithAView.txt' )\n",
    "\n",
    "novel = tdm.text(path)\n",
    "kwic = novel.concordance( 'florence' , 20 )\n",
    "\n",
    "out = open('concordance.txt' , 'w' , encoding = 'utf-8')\n",
    "\n",
    "for fragment in kwic:\n",
    "    print(fragment)\n",
    "    out.write(fragment + '\\n' )\n",
    "    \n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocation analysis\n",
    "\n",
    "Collocation analyses focus on the words that are used in the vicinity of a provided search term. It may be viewed as an extension of the principle underlying the creation of concordances. To perform a collocation analysis, we can look at the environments of a search term through a 'window' consisting of a given number of words. The words that are used in this context can obviously be counted. As the result of a collocation analysis, we can show a overview listing the words that are used most frequently in the neighbourhood of a given word. \n",
    "\n",
    "A collocation analysis can be performed using the `collocation()` method in the `tdm` module. The parameters are the same as those of the `concordance()` method: (1) a search term and (2) a number representing the size of the window (or, ot be more precise: the number of words). This function returns a dictionary listing all the words found near the search term that is provided, together with the frequencies of these words. To study the code created to make the `collocation()` method, you can obviously open the `tdm.py` file.  \n",
    "\n",
    "The function `removeStopWords()` can also be useful in this context. It removes the stopwords a given dictionary. This function makes use of the list of stopwords defined in the `nltk` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tdm\n",
    "from os.path import join\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "\n",
    "def removeStopWords( word_dict ):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    filtered = dict()\n",
    "\n",
    "    for w in word_dict:\n",
    "            if w not in stopwords_list:\n",
    "                filtered[w] = word_dict[w]\n",
    "\n",
    "    return filtered\n",
    "\n",
    "            \n",
    "dir = 'Corpus'\n",
    "file = 'ARoomWithAView.txt'\n",
    "\n",
    "novel = tdm.text( join( dir,file ) )\n",
    "freq = novel.collocation( 'florence' , 10 )\n",
    "freq = removeStopWords(freq)\n",
    "\n",
    "            \n",
    "count = 0 \n",
    "max = 30 \n",
    "\n",
    "for word in reversed( tdm.sortedByValue(freq) ):\n",
    "    count += 1\n",
    "    print( f'{ count }. { word } => { freq[word]}' )\n",
    "    if count == max:\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
