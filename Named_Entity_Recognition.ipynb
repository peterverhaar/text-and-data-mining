{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "\n",
    "A named entity is an object, person, location or organization which has been assigned a proper name. *Named Entity Recognition* is a computational technique that seeks to identify all the named entities that that are mentioned within texts. Applications making use of Named Entity Recognition can generally extract the most of the occurrences of such named entities and it can also characterise such entities using pre-defined categories such as ‘Person’, ‘Location’, ‘Work of Art’, ‘Organisation’. \n",
    "\n",
    "Named Entity Recognition applications typically make use of statistical models created using Machine Learning algorithms. Such models are often trained using large numbers of texts in which all the people, locations, organisations and named objects have been labelled manually by human readers. On the basis of a meticulous analysis of the nature and the contexts of all of these named entitities, computers can eventually be enabled to recognise similar types of entitities in new, unlabelled texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "This notebook explains how you can work with Named Entity Recognition using an NLP library named *spaCy*. For more information on how to install *spaCy*, or on how to load specific langauge models, please read the notebook on *NLP*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*spaCy* offers support for a wide range of languages. The model for the English language was trained on the basis of an large annotated corpus named *[OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19)*. This model can be downloaded using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been downloaded, it needs to be  loaded, so that you can work with it in your code. The `load()` method in `spaCy` creates a new object which can be used to add linguistic and semantic annotations. Ii the code below, is object is given the name `nlp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `nlp` object can annotate a given string in a number of ways. SpaCy can be used not only to describe properties such as the parts of speech and the lemmatised versions of words, but also find the named entities. \n",
    "In the code below, the output of the `nlp()` method is assigned to a variable named `tagged_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text = nlp(\"James Joyce (born February 2, 1882, Dublin, Ireland - died January 13, 1941, Zürich, Switzerland) was Irish novelist noted for his experimental use of language and exploration of new literary methods in such large works of fiction as Ulysses (1922) and Finnegans Wake (1939).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags added by `nlp()` can be visualised effectively using the `render()` method from the `displacy` module. When you add the parameter `style`, with value `ent`, this visulation concentrates on the named entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(tagged_text, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*spaCy* works with the following pre-defined NER labels: \n",
    "\n",
    "* PERSON: People, including fictional \n",
    "* NORP: Nationalities or religious or political groups \n",
    "* FAC: Buildings, airports, highways, bridges, etc. \n",
    "* ORG: Companies, agencies, institutions, etc. \n",
    "* GPE: Countries, cities, states \n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water \n",
    "* PRODUCT: Objects, vehicles, foods, etc. (not services) \n",
    "* EVENT: Named hurricanes, battles, wars, sports events, etc. \n",
    "* WORK_OF_ART: Titles of books, songs, etc. \n",
    "* LAW: Named documents made into laws. \n",
    "* LANGUAGE: Any named language \n",
    "* DATE: Absolute or relative dates or periods \n",
    "* TIME: Times smaller than a day \n",
    "* PERCENT: Percentage, including \"%\" \n",
    "* MONEY: Monetary values, including unit \n",
    "* QUANTITY: Measurements, as of weight or distance \n",
    "* ORDINAL: \"first\", \"second\", etc. \n",
    "* CARDINAL: Numerals that do not fall under another type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of specifc *spaCy* codes can be found the `explain()` method, as is demonstated in the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "\n",
    "for t in tags: \n",
    "    print( f'{t}: {spacy.explain(t)} ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding NER tags in longer texts\n",
    "\n",
    "One important limitation of the *spaCy* tagger is that it can only be applied to texts consisting of less than 1,000,000 characters. The parser roughly requires 1GB of memory per 100,000 characters, and texts containing more than 1,000,000 characters tends to cause memory allocation errors. \n",
    "\n",
    "The code below tries to avoid such errors. It safely sets the `max_length` of the texts to be parsed to 500,000. The code divides the full text into segments, each of which are shorter than this `max_length`.  \n",
    "\n",
    "After this, these shorter segments are all parsed one by one. These tagged texts are stored in a dictionary named `tagged_segments`. \n",
    "\n",
    "Tagging texts of ca. 500,000 characters still demands quite some memory space. The code below may take some time to complete because of this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "segments = dict()\n",
    "tagged_segments = dict()\n",
    "segment_nr = 0\n",
    "\n",
    "text = 'ARoomWithAView.txt'\n",
    "dir = 'Corpus'\n",
    "path = join( dir, text )\n",
    "max_length = 500000\n",
    "\n",
    "with open(path, encoding = 'utf-8') as file_handler:\n",
    "    full_text = file_handler.read()\n",
    "    \n",
    "print( f'Total number of characters in {text}: {len(full_text)}' )\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "length = 0 \n",
    "segment = ''\n",
    "\n",
    "for s in sentences:\n",
    "    length += len(s)\n",
    "    if length < max_length:\n",
    "        segment += s + ' '\n",
    "    else:\n",
    "        segments[segment_nr] = segment\n",
    "        segment = s + ' '\n",
    "        length = 0 \n",
    "        segment_nr += 1\n",
    "        \n",
    "if len(segment) > 0:\n",
    "    segments[segment_nr] = segment\n",
    "    \n",
    "\n",
    "print( 'Annotating the text segments ... ')    \n",
    "for i in segments:\n",
    "    print(i)\n",
    "    tagged_segments[i] = nlp(segments[i])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated texts can be analysed in a variety of ways. The code below lists the personal names that are mentioned most frequently in the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import sortedByValue\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "for doc in tagged_segments:\n",
    "    for named_entity in tagged_segments[doc].ents:\n",
    "        if named_entity.label_ == 'PERSON':\n",
    "            name = str(named_entity) \n",
    "            name = name.strip()\n",
    "            freq[ name ] = freq.get( name , 0 ) + 1\n",
    "        \n",
    "for name in reversed( sortedByValue(freq) ):\n",
    "    print( f'{name}: {freq[name]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all the works of art that are referred to in the texts, for instance, you need to replace the label 'PERSON' in the code below with the tag 'WORK_OF_ART'.\n",
    "\n",
    "The NER parser, as mentioned, also tried to identify the locations that are mentioned in the text. The locations are assigned the code 'GPE'. Once we have identified the geographical locations that are mentioned in a text, we can also try to visualise all of these locations on a map. \n",
    "\n",
    "The code below firstly stores all the locations that are found by *scraPy* in a list named `locations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import sortedByValue\n",
    "\n",
    "freq = dict\n",
    "locations = []\n",
    "\n",
    "for doc in tagged_segments:\n",
    "    for named_entity in tagged_segments[doc].ents:\n",
    "        if named_entity.label_ == 'GPE':\n",
    "            name = str(named_entity) \n",
    "            name = name.strip()\n",
    "            locations.append( name )\n",
    "        \n",
    "locations = list( set(locations) )\n",
    "\n",
    "for l in locations:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Named Entity Recognition process certainly does not function flawlessly. It probabaly misses some of the locations that are mentioned in the text, and it is also likely that it will have labelled some non-locations as locations. If necessary, you can edit the `locations` list that is created manually, making us of the `remove()` method, for instance. \n",
    "\n",
    "The following code tries to find the geographic coordinates (i.e. the longitude and the latitude) of the items listed in `locations`, using the `openStreetMap` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import requests\n",
    "\n",
    "def remove_brackets(text):\n",
    "    text = re.sub( '(\\[)|(\\])' , '' , text )\n",
    "    return text\n",
    "\n",
    "locations_coord = dict()\n",
    "    \n",
    "for loc in locations:\n",
    "    if loc not in locations_coord:\n",
    "        url = 'https://nominatim.openstreetmap.org/search?q='+ loc + '&format=xml'\n",
    "        url = re.sub( '\\s+' , '%20' , url )\n",
    "\n",
    "        response = requests.get( url )\n",
    "        root = ET.fromstring( response.text )\n",
    "        el = root.findall('place')\n",
    "\n",
    "        count = 0\n",
    "        if el is not None:\n",
    "            for place in el:\n",
    "                count += 1\n",
    "                lat = place.attrib['lat']\n",
    "                lon = place.attrib['lon']\n",
    "                if count == 1:\n",
    "                    locations_coord[ loc ] = ( lat , lon )\n",
    "                    print(f'{loc}: {lat} {lon}')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the locations are drawn on a map using `Leaflet`, A Javascript library for creating interactive maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open( 'map.html' , 'w' , encoding = 'utf-8')\n",
    "import re\n",
    "\n",
    "out.write('''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\n",
    "                <title>Locations</title>\n",
    "\n",
    "                <meta charset=\"utf-8\" />\n",
    "                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "\n",
    "                <link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"docs/images/favicon.ico\" />\n",
    "\n",
    "    <link rel=\"stylesheet\" href=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.css\" integrity=\"sha512-xodZBNTC5n17Xt2atTPuE1HxjVMSvLVW9ocqUKLsCC5CXdbqCmblAshOMAS6/keqq/sMZMZ19scR4PsZChSR7A==\" crossorigin=\"\"/>\n",
    "    <script src=\"https://unpkg.com/leaflet@1.7.1/dist/leaflet.js\" integrity=\"sha512-XQoYMqMTK8LvdxXYG3nZ448hOEQiglfqkJs1NOQV44cWnUrBc8PkAOcXy20w0vlaXaVUearIOBhiXZ5V3ynxwA==\" crossorigin=\"\"></script>\n",
    "\n",
    "\n",
    "\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div id=\"mapid\" style=\"width: 600px; height: 400px;\"></div>\n",
    "<script>\n",
    "\n",
    "                var mymap = L.map('mapid').setView([52.0799838, 4.3113461], 4);\n",
    "\n",
    "                L.tileLayer('https://api.mapbox.com/styles/v1/{id}/tiles/{z}/{x}/{y}?access_token=pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw', {\n",
    "                                maxZoom: 18,\n",
    "                                attribution: 'Map data &copy; <a href=\"https://www.openstreetmap.org/\">OpenStreetMap</a> contributors, ' +\n",
    "                                                '<a href=\"https://creativecommons.org/licenses/by-sa/2.0/\">CC-BY-SA</a>, ' +\n",
    "                                                'Imagery  <a href=\"https://www.mapbox.com/\">Mapbox</a>',\n",
    "                                id: 'mapbox/streets-v11',\n",
    "                                tileSize: 512,\n",
    "                                zoomOffset: -1\n",
    "                }).addTo(mymap); \n",
    "''')\n",
    "\n",
    "for l in locations_coord:\n",
    "    display_name = re.sub( '\\'' , '' , l )\n",
    "    out.write( f' L.marker([ { locations_coord[l][0] }, { locations_coord[l][1] }  ]).addTo(mymap) ')\n",
    "    out.write( f\" .bindPopup('{display_name}.') \")  \n",
    "    out.write( ';' )\n",
    "    \n",
    "out.write(\n",
    "'''\n",
    "</script>\n",
    "\n",
    "\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "''')\n",
    "\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "from os.path import exists\n",
    "\n",
    "IFrame(src= 'map.html' , width=700, height=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
